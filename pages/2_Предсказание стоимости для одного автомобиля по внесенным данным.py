import streamlit as st
import pandas as pd
import numpy as np
from sklearn.feature_extraction import FeatureHasher
import pickle
from pathlib import Path

st.set_page_config(page_title="–ú–æ–¥–µ–ª—å –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è —Å—Ç–æ–∏–º–æ—Å—Ç–∏ –∞–≤—Ç–æ–º–æ–±–∏–ª—è –Ω–∞ csv-–¥–∞–Ω–Ω—ã—Ö", page_icon="üéØ", layout="wide")

MODEL_DIR = Path(__file__).resolve().parent.parent / "models_artefacts"
DATA_DIR = Path(__file__).resolve().parent.parent / "data"
MODEL_PATH = MODEL_DIR / "model.pkl"
SCALER_PATH = MODEL_DIR / "scaler.pkl"
ENCODER_PATH = MODEL_DIR / "ohe_encoder.pkl"
TRAIN_DATA_PATH = DATA_DIR / "df_train.parquet"

# —Å–ø–∏—Å–æ–∫ –ø—Ä–∞–≤–∏–ª—å–Ω—ã—Ö –∫–æ–ª–æ–Ω–æ–∫ 
NAME_COLS_IN_TRAIN = [
    "year",
    "km_driven",
    "mileage",
    "engine",
    "max_power",
    "torque",
    "max_torque_rpm",
    "seats",
    "name"
]

# —Ñ—É–Ω–∫—Ü–∏—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
def check_data(df, name_cols):
    """
    –ü—Ä–æ–≤–µ—Ä–∫–∞ –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –Ω–∞ –Ω–∞–ª–∏—á–∏–µ –æ—à–∏–±–æ–∫
    """

    # –ø—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –ø—É—Å—Ç—ã—Ö —Å—Ç—Ä–æ–∫
    if df.empty:
        st.error("üòï –í –≤–∞—à–µ–º —Ñ–∞–π–ª–µ –Ω–µ—Ç –¥–∞–Ω–Ω—ã—Ö!")
        st.stop()
    
    # –ø—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –ø—É—Å—Ç—ã—Ö —Å—Ç–æ–ª–±—Ü–æ–≤
    if df.columns.empty:
        st.error("üòï –í –≤–∞—à–µ–º —Ñ–∞–π–ª–µ –Ω–µ—Ç —Å—Ç–æ–ª–±—Ü–æ–≤!")
        st.stop()

    if df.isnull().values.any():
        st.error("üòï –ó–∞–ø–æ–ª–Ω–µ–Ω—ã –Ω–µ –≤—Å–µ –ø–æ–ª—è!")        
        st.stop()   

    if not set(df.columns).issubset(set(name_cols)):
        st.error(f"üòï –í –≤–∞—à–µ–º —Ñ–∞–π–ª–µ –µ—Å—Ç—å –Ω–µ–¥–æ–ø—É—Å—Ç–∏–º—ã–µ —Å—Ç–æ–ª–±—Ü—ã! {df.columns}")
        st.stop()

    if len(df.columns) != len(name_cols):
        st.error(f"üòï –í –≤–∞—à–µ–º —Ñ–∞–π–ª–µ –µ—Å—Ç—å –Ω–µ–¥–æ–ø—É—Å—Ç–∏–º—ã–µ —Å—Ç–æ–ª–±—Ü—ã! {df.columns}")
        st.stop()


@st.cache_resource
def load_model():
    """–ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å –∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –æ–±—Ä–∞–±–æ—Ç—á–∏–∫–∏ –¥–∞–Ω–Ω—ã—Ö —á–µ—Ä–µ–∑ pickle"""

    with open(MODEL_PATH, 'rb') as f:
        model = pickle.load(f)

    return model


def prepare_features(df, df_train):
    """–ü—Ä–∏–≤–æ–¥–∏–º –¥–∞–Ω–Ω—ã–µ –∫ —Ñ–æ—Ä–º–∞—Ç—É –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏."""
    top_n = 20
    hashing_n_features = 10
    target_col = 'selling_price'
    df_proc = df.copy()

    # –ó–∞–≥—Ä—É–∑–∫–∞ scaler
    try:
        with open(SCALER_PATH, 'rb') as f:
            scaler = pickle.load(f)
    except Exception as e:
        st.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∞—Ç–æ—Ä–∞: {e}")
        st.stop()

    # –ó–∞–≥—Ä—É–∑–∫–∞ ohe encoder
    try:
        with open(ENCODER_PATH, 'rb') as f:
            encoder = pickle.load(f)
    except Exception as e:
        st.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –æ–±—Ä–∞–±–æ—Ç—á–∏–∫–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {e}")
        st.stop()   

    # –ø—Ä–∏–≤–µ–¥–µ–º –∫–æ–ª–æ–Ω–∫–∏ –∫ —Å—Ç—Ä–æ–≥–æ–º—É –ø–æ—Ä—è–¥–∫—É –∫–∞–∫ –≤ –æ–±—É—á–µ–Ω–∏–∏
    df_proc = df_proc[NAME_COLS_IN_TRAIN]

    # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –∫ float
    df_proc['mileage'] = df_proc['mileage'].astype(float)
    df_proc['engine'] = df_proc['engine'].astype(float)
    df_proc['max_power'] = df_proc['max_power'].astype(float)
    df_proc['torque'] = df_proc['torque'].astype(float)
    df_proc['max_torque_rpm'] = df_proc['max_torque_rpm'].astype(float)

    # –ø—Ä–∏–≤–µ–¥–µ–º –∫–æ–ª–æ–Ω–∫–∏ engine –∏ seats –∫ —Ç–∏–ø—É int
    df_proc['seats'] = df_proc['seats'].astype(int)
    df_proc['engine'] = df_proc['engine'].astype(int)

    # Frequency Encoding
    freq = df_train['name'].value_counts()
    df_proc['name_freq'] = df_proc['name'].map(freq)
    # –µ—Å–ª–∏ –≤ —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–µ –Ω–µ—Ç —Ç–∞–∫–æ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –∑–∞–ø–æ–ª–Ω—è–µ–º –ø—Ä–æ–ø—É—Å–∫ 0
    df_proc['name_freq'] = df_proc['name_freq'].fillna(0)
    # –ª–æ–≥–∞—Ä–∏—Ñ–º–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —á–∞—Å—Ç–æ—Ç–∞
    df_proc['name_freq_log'] = np.log1p(df_proc['name_freq'])
    
    # Top-N –∫–∞—Ç–µ–≥–æ—Ä–∏–π
    top_cats = freq.head(top_n).index
    
    # One-hot –¥–ª—è —Ç–æ–ø-N
    for i, cat in enumerate(top_cats, 1):
        df_proc[f'name_cat_{i:02d}'] = (df_proc['name'] == cat).astype(int)
    
    # Target Encoding 
    if target_col and target_col in df_train.columns:
        # –°–≥–ª–∞–∂–µ–Ω–Ω–æ–µ —Å—Ä–µ–¥–Ω–µ–µ
        global_mean = df_train[target_col].mean()
        # –∫–æ—ç—Ñ. —Å–≥–ª–∞–∂–∏–≤–∞–Ω–∏—è
        smoothing = 100
        
        def smoothed_target(group):
            '''–§—É–Ω–∫—Ü–∏—è –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–≥–ª–∞–∂–µ–Ω–Ω–æ–µ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ target encoding'''
            n = len(group)
            if n == 0:
                return global_mean
            group_mean = group.mean()
            return (n * group_mean + smoothing * global_mean) / (n + smoothing)

        # –ø–æ—Å—á–∏—Ç–∞–µ–º target encoder
        target_map = df_train.groupby('name')[target_col].apply(smoothed_target).to_dict()
        # –ø—Ä–∏–º–µ–Ω–∏–º –Ω–∞ —Ç—Ä–µ–π–Ω –∏ —Ç–µ—Å—Ç –≤—ã–±–æ—Ä–∫–µ
        df_proc['name_target'] = df_proc['name'].map(target_map)
        # –µ—Å–ª–∏ –≤ —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–µ –Ω–µ—Ç —Ç–∞–∫–æ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –∑–∞–ø–æ–ª–Ω—è–µ–º –ø—Ä–æ–ø—É—Å–∫ –ø—Ä–æ—Å—Ç–æ –≥–ª–æ–±–∞–ª—å–Ω—ã–º —Å—Ä–µ–¥–Ω–∏–º
        df_proc['name_target'] = df_proc['name_target'].fillna(global_mean)

    # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º —Å—Ç—Ä–æ–∫–∏ –≤ —Ñ–æ—Ä–º–∞—Ç –¥–ª—è —Ö–µ—à–∏—Ä–æ–≤–∞–Ω–∏—è
    name_strings_test = [[str(x)] for x in df_proc['name'].values]
    
    # –°–æ–∑–¥–∞–µ–º FeatureHasher
    hasher = FeatureHasher(n_features=hashing_n_features, input_type='string')
    
    # –ü—Ä–∏–º–µ–Ω—è–µ–º —Ö–µ—à–∏—Ä–æ–≤–∞–Ω–∏–µ
    hashed_features_test = hasher.transform(name_strings_test).toarray()
    
    hashed_test_df = pd.DataFrame(
        hashed_features_test,
        columns=[f'name_hash_{i}' for i in range(hashing_n_features)],
        index=df_proc.index
    )
    
    # –î–æ–±–∞–≤–ª—è–µ–º –∫ –æ—Å–Ω–æ–≤–Ω–æ–º—É DataFrame
    df_proc = pd.concat([df_proc, hashed_test_df], axis=1)

    # –¢–∞–∫–∂–µ —Å–æ–∑–¥–∞–µ–º –∞–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –∏–∑ —Ö–µ—à–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö
    df_proc['name_hash_sum'] = hashed_features_test.sum(axis=1)
    df_proc['name_hash_mean'] = hashed_features_test.mean(axis=1)
    df_proc['name_hash_std'] = hashed_features_test.std(axis=1)
    
    # –£–¥–∞–ª–∏–º –∏—Å—Ö–æ–¥–Ω—É—é –∫–æ–ª–æ–Ω–∫—É
    df_proc = df_proc.drop(columns=['name'])

    # –ü—Ä–∏–º–µ–Ω–∏–º ohe —ç–Ω–∫–æ–¥–µ—Ä –Ω–∞ —Ç–µ—Å—Ç –≤—ã–±–æ—Ä–∫–µ
    encoded_test_array = encoder.transform(df_proc['seats'].values.reshape(-1, 1))

    # –ü–æ–ª—É—á–∏–º –∏–º–µ–Ω–∞ –Ω–æ–≤—ã—Ö –∫–æ–ª–æ–Ω–æ–∫
    feature_names = encoder.get_feature_names_out(['seats'])

    # –°–æ–∑–¥–∞–µ–º DataFrame —Å –∑–∞–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏ test
    encoded_test_df = pd.DataFrame(
        encoded_test_array,
        columns=feature_names,
        index=df_proc.index
    )

    # –î–æ–±–∞–≤–ª—è–µ–º –∫ –æ—Å–Ω–æ–≤–Ω–æ–º—É DataFrame
    X_test_cat = pd.concat([df_proc, encoded_test_df], axis=1)
    
    # –£–¥–∞–ª–∏–º –∫–æ–ª–æ–Ω–∫—É `seats` 
    X_test_cat = X_test_cat.drop(columns=['seats'])

    # —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∏—Ä—É–µ–º –≤—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
    X_test_scaled = scaler.transform(X_test_cat)

    return X_test_scaled


# –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å
try:
    MODEL = load_model()
except Exception as e:
    st.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏: {e}")
    st.stop()


# --- –û—Å–Ω–æ–≤–Ω–æ–π –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å ---
st.title("üéØ –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —Å—Ç–æ–∏–º–æ—Å—Ç–∏ –∞–≤—Ç–æ–º–æ–±–∏–ª—è –ø–æ –≤–≤–µ–¥–µ–Ω–Ω—ã–º –¥–∞–Ω–Ω—ã–º")

# --- –§–æ—Ä–º–∞ –¥–ª—è –∑–∞–ø–æ–ª–Ω–µ–Ω–∏—è –∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è ---
st.subheader("üîÆ –°–¥–µ–ª–∞—Ç—å –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –¥–ª—è –æ–¥–Ω–æ–≥–æ –∞–≤—Ç–æ–º–æ–±–∏–ª—è")

with st.form("prediction_form"):
    col_left, col_right = st.columns(2)
    input_data = {}
    
    with col_left:
        st.write("**–ß–∏—Å–ª–æ–≤—ã–µ:**")
        for col in ["year", "seats", "mileage", "km_driven", "engine", "max_power", "torque", "max_torque_rpm"]:
            input_data[col] = st.number_input(col, min_value=0, key=col)

    with col_right:
        st.write("**–ö–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ:**")
        for col in ["name"]:
            input_data[col] = st.text_input(
                "–í–≤–µ–¥–∏—Ç–µ –Ω–∞–∑–≤–∞–Ω–∏–µ –∞–≤—Ç–æ–º–æ–±–∏–ª—è",
                placeholder="–ù–∞–ø—Ä–∏–º–µ—Ä: Honda Civic 1.8 S AT",
                key=col
            )

    submitted = st.form_submit_button("–ü—Ä–µ–¥—Å–∫–∞–∑–∞—Ç—å", use_container_width=True)

if submitted:
    try:
        input_df = pd.DataFrame([input_data])
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
        check_data(input_df, NAME_COLS_IN_TRAIN)
        # –∑–∞–≥—Ä—É–∂–∞–µ–º —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏ –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        train_df = pd.read_parquet(TRAIN_DATA_PATH)
        features = prepare_features(input_df, train_df)
        prediction = round(np.expm1(MODEL.predict(features))[0], 2)

        st.success(f"**–†–µ–∑—É–ª—å—Ç–∞—Ç:** {prediction} y.e.")
    except Exception as e:
        st.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–∏: {e}")